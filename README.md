# Greedy Layer-wise Network Architecture Search (GLNAS)
## Introduction
GLNAS trains network layers one after another, and the performance of the network is evaluated after each layer is added.
If the performance of the trained network with a newly added hidden layer is lower than the one trained with the previous hidden layer, training is stopped and the network architecture with the previous trained hidden layer is selected as a final network architecture; thus allowing for a flexible number of hidden layers used in the network during training.
Additionally, GLNAS evaluates the performance of skip connections by feeding the input of the current hidden layer with outputs from different previous hidden layers and choose the best skip connection among them in order to achieve highly robust network.

## Usages
Please use tf/GLNAS-cifar10.py and tf/GLNAS-cifar100.py to train CIFAR10 and CIFAR100 dataset respectively.

Please use test.py and test_cifar100.py to test the final model generated by GLNAS with CIFAR10 and CIFAR100 dataset respectively.
